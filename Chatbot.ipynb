{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN3DGZyRSd4m2PNdUp2yR0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GehrmanSparrow116/DJS-CODE-AI/blob/main/Chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QAI0Ph8U_lvE",
        "outputId": "d1766df1-a994-4d1a-848c-4d448686ca69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2754' max='2754' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2754/2754 07:02, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.472900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.618700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.658900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.682900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.607500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.679700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.419300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.456200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.345900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.535400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.382700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.371900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.183500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.355700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.217200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.214300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.196500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.156500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.242700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter prompt (or type 'exit' to quit): warrior\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Text:\n",
            " warrior – chief – chief – officer of the Archaeological Survey of India ( Archaeological Survey ) , Kurthasar Gupta of Karnataka , claimed that the archaeological record indicates that the archaeologist 's position in this period was ' the same , but that the archaeologist had a different opinion regarding the occurrence of archaeology in Karnataka . ' He claimed that the archaeological record indicates that the archaeologist 's position in this period was ' the same , but that the archaeologist 's position\n",
            "Enter prompt (or type 'exit' to quit): who am i\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Generated Text:\n",
            " who am i the sole god , that is the divine creation of the whole world , is the creation of the whole world . \n",
            "\n",
            "Enter prompt (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:10%]\")\n",
        "train_texts = dataset['text'][:10000]\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token  # set EOS as padding\n",
        "tokenized_texts = [tokenizer(t, truncation=True, padding=\"max_length\", max_length=64)\n",
        "                   for t in train_texts]\n",
        "import torch\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, tokenized_texts):\n",
        "        self.input_ids = [torch.tensor(t['input_ids']) for t in tokenized_texts]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {'input_ids': self.input_ids[idx], 'labels': self.input_ids[idx]}\n",
        "\n",
        "dataset = TextDataset(tokenized_texts)\n",
        "\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\")\n",
        "\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mini_gpt\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=100,\n",
        "    learning_rate=5e-5,\n",
        "    fp16=True,\n",
        "    report_to=[]  # disables all logging backends (wandb, tensorboard)\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset\n",
        ")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "\n",
        "while True:\n",
        "    prompt = input(\"Enter prompt (or type 'exit' to quit): \")\n",
        "    if prompt.lower() == \"exit\":\n",
        "        break\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=100,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.8\n",
        "    )\n",
        "    print(\"\\nGenerated Text:\\n\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DbDglbj0sd_9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}